
import os
import sys
import logging
from datetime import datetime

import pandas as pd
import pickle

from airflow import DAG
from airflow.decorators import dag, task
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import BranchPythonOperator
from airflow.operators.latest_only import LatestOnlyOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook

import torch
import torchvision
from torch.utils.data import random_split
from torch.utils.data.dataloader import DataLoader
from torchvision.utils import make_grid

sys.path.append('/model')
import model as classifier
import gpu


logger = logging.getLogger(__name__)
logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')


def _check_files(**kwargs):
    file_path = "/data/ml-data.zip"

    print(os.path.exists(file_path))
    if os.path.exists(file_path):
        return "transform_data"
    else:
        return "download_from_s3"
    
    
def _download_from_s3(key: str, bucket_name: str, local_path: str) -> str:
    hook = S3Hook('aws_conn')
    file_name = hook.download_file(
        key=key,
        bucket_name=bucket_name,
        local_path=local_path,
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )


def _transform_data(input_path, **context):
    transform = torchvision.transforms.Compose([
        torchvision.transforms.Resize((64, 64)),
        torchvision.transforms.ToTensor()
    ])
        
    dataset = torchvision.datasets.ImageFolder(input_path, transform=transform)
    
    
    with open("/pickle/dataset.pickle", 'wb') as pickle_file:
        pickle.dump(dataset, pickle_file) 


def _split_dataset(random_seed, val_size, test_size, **context):
    
    with open("/pickle/dataset.pickle", "rb") as pickle_file:
        dataset = pickle.load(pickle_file)
    
    
    torch.manual_seed(random_seed)
    
    train_size = len(dataset) - val_size - test_size
    lengths = [train_size, val_size, test_size]
    
    train_ds, val_ds, test_ds = random_split(dataset, lengths)
    
    splited_datasets = (train_ds, val_ds, test_ds)
    
    
    with open("/pickle/splited_dataset.pickle", "wb") as pickle_file:
        pickle.dump(splited_datasets, pickle_file)


def _build_dataloaders(batch_size, **context):
    
    with open("/pickle/splited_dataset.pickle", "rb") as pickle_file:
        splited_dataset = pickle.load(pickle_file)
        train_ds, val_ds, _ = splited_dataset
    
    
    train_dl = DataLoader(
        dataset= train_ds,
        batch_size= batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    
    val_dl = DataLoader(
        dataset= val_ds,
        batch_size= batch_size*2,
        num_workers=0,
        pin_memory=True
    )
    
    dataloaders = (train_dl, val_dl)
    
    
    with open("/pickle/dataloaders.pickle", "wb") as pickle_file:
        pickle.dump(dataloaders, pickle_file)


def _initialize_model():
    model = classifier.SatelliteClassificationModel()
    with open("/pickle/model.pickle", "wb") as pickle_file:
        pickle.dump(model, pickle_file)


def _setup_gpu():
    
    with open("/pickle/model.pickle", "rb") as pickle_file:
        model = pickle.load(pickle_file)
        
    with open("/pickle/dataloaders.pickle", "rb") as pickle_file:
        train_dl, val_dl = pickle.load(pickle_file)
    
    
    device = gpu.get_default_device()
    
    train_dl = gpu.DeviceDataLoader(train_dl, device)
    val_dl = gpu.DeviceDataLoader(val_dl, device)
    dataloaders = (train_dl, val_dl)
    
    gpu.to_device(model, device)
    
    
    with open("/pickle/model.pickle", "wb") as pickle_file:
        pickle.dump(model, pickle_file)

    with open("/pickle/dataloaders.pickle", "wb") as pickle_file:
        pickle.dump(dataloaders, pickle_file)


@torch.no_grad()
def _evaluate():
    
    with open("/pickle/model.pickle", "rb") as pickle_file:
        model = pickle.load(pickle_file)
        
    with open("/pickle/dataloaders.pickle", "rb") as pickle_file:
        _, val_loader = pickle.load(pickle_file)
    
    
    # logger.info(list(model.parameters())[0].device)
    
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    
    logger.info(model.validation_epoch_end(outputs))
    
    return model.validation_epoch_end(outputs)


def _fit(epochs, lr, opt_func=torch.optim.SGD):
    
    with open("/pickle/model.pickle", "rb") as pickle_file:
        model = pickle.load(pickle_file)
        
    with open("/pickle/dataloaders.pickle", "rb") as pickle_file:
        train_loader, val_loader = pickle.load(pickle_file)
    
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        
        # Training Phase
        model.train()
        train_losses = []
        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        
        # Validation phase
        model.eval()
        outputs = [model.validation_step(batch) for batch in val_loader]
        result = model.validation_epoch_end(outputs)
    
        result['train_loss'] = torch.stack(train_losses).mean().item()
        model.epoch_end(epoch, result)
        
        logger.info(result)
        
        history.append(result)
    
        with open("/pickle/history.pickle", "wb") as pickle_file:
            pickle.dump(history, pickle_file)


def _export_metrics_to_db():
    
    with open("/pickle/history.pickle", "rb") as pickle_file:
        history = pickle.load(pickle_file)
    
    postgres_hook = PostgresHook(postgres_conn_id="metabase_postgres", schema="metabase")
    
    df = pd.DataFrame(history)
    
    df["model_id"] = 1
    df["train_id"] = 1
    df.reset_index(inplace=True)
    df.rename(columns={"index": "epoch", "val_loss": "loss", "val_acc": "accuracy"}, inplace= True)
    df["epoch"] = df["epoch"] + 1
    df = df[['model_id', 'train_id', 'epoch', 'loss', 'accuracy', 'train_loss']]
    
    
    df.to_sql(
        name='performance_metrics',
        con= postgres_hook.get_sqlalchemy_engine(),
        schema='machine_learning',
        if_exists='replace',
        chunksize=1000
    )


def _save_module(filename):
    
    with open("/pickle/model.pickle", "rb") as pickle_file:
        model = pickle.load(pickle_file)
        
    torch.save(model.state_dict(), filename)


def _export_model_to_s3():
    
    hook = S3Hook('aws_conn')
    
    hook.load_file(
        filename='satellite-classification-model.pth',
        bucket_name='ml-training-inputs',
        key='satellite-classification-model.pth'
    )


with DAG(
    dag_id="satellite-imagery-classification",
    start_date=datetime(2019, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:

    check_files = BranchPythonOperator (
        task_id='check_files',
        python_callable= _check_files
    )


    download_from_s3 = PythonOperator (
        task_id= "download_from_s3",
        python_callable= _download_from_s3,
        op_kwargs={
            'key': 'ml-data.zip',
            'bucket_name': 'ml-training-inputs',
            'local_path': '/data',
        }
    )

    unzip_file = BashOperator(
        task_id="unzip_file",
        bash_command='unzip /data/ml-data.zip -d /data'
    )


    transform_data = PythonOperator(
        task_id= "transform_data",
        python_callable= _transform_data,
        op_kwargs={
            'input_path': '/data/ml_data',
        },
        trigger_rule='one_success'
    )


    split_dataset = PythonOperator(
        task_id= "split_dataset",
        python_callable= _split_dataset,
        op_kwargs={
            'random_seed': 30,
            'val_size': 700,
            'test_size': 900,
        }
    )


    build_dataloaders = PythonOperator(
        task_id= "build_dataloaders",
        python_callable= _build_dataloaders,
        op_kwargs={
            'batch_size': 128,
        }
    )


    initialize_model = PythonOperator(
        task_id= "initialize_model",
        python_callable= _initialize_model
    )


    setup_gpu = PythonOperator(
        task_id= "setup_gpu",
        python_callable= _setup_gpu
    )


    # evaluate = PythonOperator(
    #     task_id= "evaluate",
    #     python_callable= _evaluate
    # )


    train_model = PythonOperator(
        task_id = "train_model",
        python_callable=_fit,
        op_kwargs={
            'epochs': 10,
            'lr': 0.001,
            'opt_func': torch.optim.Adam
        }
    )


    export_metrics_to_db = PythonOperator(
        task_id= "export_metrics_to_db",
        python_callable= _export_metrics_to_db,
    )


    save_model = PythonOperator(
        task_id = "save_model",
        python_callable=_save_module,
        op_kwargs={
            'filename': 'satellite-classification-model.pth',
        }
    )

    export_model_to_s3 = PythonOperator (
        task_id= "export_model_to_s3",
        python_callable= _export_model_to_s3,
    )

    check_files >> [transform_data, download_from_s3]
    download_from_s3 >> unzip_file >> transform_data

    transform_data >> split_dataset >> build_dataloaders >> setup_gpu
    initialize_model >> setup_gpu

    setup_gpu >> train_model >> [export_metrics_to_db, save_model]
    save_model >> export_model_to_s3

